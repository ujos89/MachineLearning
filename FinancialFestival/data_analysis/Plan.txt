#plan for new data structure

goal -> 2020년 7월에 그룹내 매수 고객수가 가장 높을 3개 종목

prediction -> 그룹내의 얼마나 많은 비율의 고객이 구매할것인가?(label)
idea- > 그룹별로 각각 모델을 적용시키자, data의 수가 많지않으니 Machine Learning을 적용시키자.

1) 데이터 가공,결측치 처리  
결측치가 있는 column 확인후 결측치 처리

2) NEWDATA SET
prediction를 하기 위해서 stocks의 column 만으로 train_se을 만들자
e.g. train_MAD01
rows : 종목번호 (stock_idx)`
columns : []:


<col[0]: stock_code> // stocks_종목번호(col[2])
종목번호 = 종목명

<col[1]: date> // stocks_기준일자(col[1])
날짜의 연속성이 영향을 미칠것인가? 그전날 다음날이 의미있을수 있다. 걍 달별로 때려박자
20190701 => 0
날짜 지날때마다 1씩 추

<col[2]: kospi> // stocks_시장구분(col[5])
kospi:1, kosdaq:0

<col[3]: standard1> // stocks_표준산업구분코드_대분류(col[6])
<col[4]: standard2> // stocks_중분류(col[7])
<col[5]: standard3> // stocks_소분류(col[8])
one-hot coding

<col[6]: price_low2high> // stocks_종목저가-종목고가(col[11]-col[10])
scaling: (high-low)/low

<col[7]: price_start2end> // stocks_종목종가-종목시가(col[12]-col[9])
scaling: (end-start)/start

<col[8]: trade_volume> // stocks_거래량(col[13])
standard_scaler
<col[9]: trade_amount> // stocks_거래금액_만원단위(col[14])
standard_scaler

<label> trade_매수고객수/그룹내고객수 (col[7]/col[3])
label : buy_prob = buy_num / group_size


<data for predict>
20년 7월 top3 대상여부 => stockcode
stockcode stocks data => crawling 

prediction -> stocks 7월 ()
option1) standard scaler를 활용해서 0,0,0,0 으로 predict
option2) 7월 stocks data를 크롤링해서 predict
-> assume, 매수 수량 및 가격은 필요없음. 얼마나 많은 고객이 매수할껀지를 확인하는 과정
-> crawling 필요없음 test data까지있음

3) Train, Test set split
cv하기 위한 준비, cv

4) Standard Sclaer
정규분포를 따른다고 가정하고, Standard scaleing

5) Model training, choose by cv

6) Prediction futrue data



====20.09.30=====
크롤링 -> stock_code가 주어진 상황에서 종목저가, 종목고가, 종목종가, 거래량, 거래금액_만원단위


